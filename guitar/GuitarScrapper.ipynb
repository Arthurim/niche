{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper for guitar selling posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script scraps both *The Gear Page* forum and the *Gear4Sale* subreddit for posts selling guitars.\n",
    "For now the below characteristics are scrapped:\n",
    "- brand\n",
    "- price\n",
    "- vintage\n",
    "- color\n",
    "- wood\n",
    "\n",
    "To do:\n",
    "- scrap model\n",
    "- create database\n",
    "- compute average/median price\n",
    "- scrap prices from other places ?\n",
    "- define if a model is \"a good buy opportunity\"\n",
    "\n",
    "It then saves the posts in a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the data is persisted to a table in kdb.\n",
    "\n",
    "- You need to launch localhost with port 41643, username and password is \"n1ch3\". Then the initialization function will take care of the rest.\n",
    "\n",
    "- The kdb is assumed to be located at C:/dev/niche/kdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to have the below path created, or to modify it for the data to be saved where you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/dev/niche/guitar/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to be able to import the below packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(section):\n",
    "    a_s=section.findAll(\"a\")\n",
    "    for a in a_s:\n",
    "        if a.has_attr('href'):\n",
    "            # the first link is the right one\n",
    "            return a[\"href\"]\n",
    "        #TODO check link is valid\n",
    "\n",
    "def validate_string(s):\n",
    "    \"\"\"\n",
    "    Returns True if:\n",
    "        - s is a string\n",
    "    &\n",
    "        - s is not an empty string\n",
    "    \"\"\"\n",
    "    is_s=False\n",
    "    if type(s)==str:\n",
    "        if s!=\"\":\n",
    "            is_s =True\n",
    "    return is_s\n",
    "\n",
    "def find_vintage(content):\n",
    "    result = \"\"\n",
    "    content = content.lower()\n",
    "    # from 1985\n",
    "    if \"from \" in content:\n",
    "        split_on_from =  content.split(\"from \")\n",
    "        if len(split_on_from)==2:\n",
    "            #\"guitar from 1985\"\n",
    "            year = split_on_from[1][:4]\n",
    "            # still check that we are returning a number\n",
    "            if year.isdigit():\n",
    "                if check_vintage(year):\n",
    "                    result = year\n",
    "        else:\n",
    "            # \"guitar from the best from 1985\"\n",
    "            for i in range(len(split_on_from)):\n",
    "                year = split_on_from[i][:4]\n",
    "                # still check that we are returning a number\n",
    "                if year.isdigit():\n",
    "                    if check_vintage(year):\n",
    "                        result = year\n",
    "    else: \n",
    "        potential_years = []\n",
    "        for i in range(len(content)):\n",
    "            if len(content[i:])>4:\n",
    "                four_char = content[i:i+4]\n",
    "                if four_char.isdigit():\n",
    "                    if check_vintage(four_char):\n",
    "                        potential_years.append(four_char)\n",
    "            if len(potential_years)==1:\n",
    "                # \"guitar blabla 1985\"\n",
    "                result=potential_years[0]\n",
    "            elif len(potential_years)>1:\n",
    "                # \"guitar made in 1985, bought in 2001\"\n",
    "                result = min(potential_years)\n",
    "    return result\n",
    "        # TODO check for \"made in\", \"manufactured in\" etc\n",
    "\n",
    "def find_longest_string_of_digits(s):\n",
    "    \"\"\"\n",
    "    from a string, assuming the first character is a digit, will return the longest number\n",
    "    eg:\n",
    "    - \"1234 frefr\" -> \"1234\"\n",
    "    - \"j 12345 p\" -> \"\"\n",
    "    \n",
    "    :s: string\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    n = len(s)\n",
    "    if n>0:\n",
    "        while (s[i].isdigit() or (s[i]==\".\") or (s[i]==\",\")) and (i<n-1):\n",
    "            i+=1\n",
    "            if (i==n):\n",
    "                break\n",
    "        return s[:i]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def find_price(content):\n",
    "    result = \"\"\n",
    "    content = content.lower()\n",
    "    if \"price is \" in content:\n",
    "        split_on_price_is = content.split(\"price is \")\n",
    "        if len(split_on_price_is)==2:\n",
    "            # price is 300,45\n",
    "            # find\n",
    "            p = find_longest_string_of_digits(split_on_price_is[1][find_ind_of_next_digit_in_string(split_on_price_is[1]):])\n",
    "            if check_price(p):\n",
    "                result = p\n",
    "        else:\n",
    "            ps = []\n",
    "            # price is 45.00. price is negotiable:\n",
    "            for i in range(len(split_on_price_is)):\n",
    "                print(split_on_price_is[i])\n",
    "                longest_digit = find_longest_string_of_digits(split_on_price_is[i])\n",
    "                if longest_digit!=\"\":\n",
    "                    if check_price(longest_digit):\n",
    "                        ps.append(longest_digit)\n",
    "            if len(ps)==1:\n",
    "                result = ps[0]\n",
    "            elif len(ps)>1:\n",
    "                result = ps[0]\n",
    "    for sign in [\"usd\", \"$\", \"price\"]:\n",
    "        if sign in content:\n",
    "            split_on_dollar_sign = content.split(sign)\n",
    "            p = find_longest_string_of_digits(split_on_dollar_sign[1][find_ind_of_next_digit_in_string(split_on_dollar_sign[1]):])\n",
    "            if p.isdigit():\n",
    "                if check_price(p):\n",
    "                    return p\n",
    "    return result\n",
    "\n",
    "def check_price(p,lb=1700,ub=50000):\n",
    "    try:\n",
    "        if type(p)==str:\n",
    "            p = float(p.replace(\",\",\"\"))\n",
    "        return p>lb  and p <ub\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(p)\n",
    "\n",
    "def check_vintage(v):\n",
    "    \"\"\"\n",
    "    vintage shd be between 1900 and this year\n",
    "    \"\"\"\n",
    "    t = datetime.datetime.today()\n",
    "    if type(v)==str:\n",
    "        v=int(v)\n",
    "    return v>1900 and v<t.year+1\n",
    "\n",
    "def find_ind_of_next_digit_in_string(s):\n",
    "    \"\"\"\n",
    "    Returns the index of the next character which is a digit\n",
    "    \n",
    "    eg:\n",
    "    - \"Ah 1\" -> 3\n",
    "    - \"2\" -> 0\n",
    "    - \"ABC\" -> None\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isdigit():\n",
    "            return i\n",
    "\n",
    "def get_brand(content):\n",
    "    brands = pd.read_csv(path+\"guitar_brands.csv\")[\"Brand\"].tolist()\n",
    "    result = \"\"\n",
    "    content = content.lower()\n",
    "    for brand in brands:\n",
    "        if brand in content:\n",
    "            return brand\n",
    "    return result\n",
    "\n",
    "def get_model(content,brand):\n",
    "    result = \"\"\n",
    "    models = pd.read_csv(path+brand+\"_models.csv\")[\"Model\"].tolist()\n",
    "    content = content.lower()\n",
    "    for model in models:\n",
    "        if model in content:\n",
    "            return model\n",
    "    return result\n",
    "    \n",
    "def find_color(s):\n",
    "    for c in [\"blue\", \"red\", \"yellow\", \"pink\", \"green\", \"purple\", \"white\", \"grey\", \"black\", \"beige\", \"orange\"]:\n",
    "        if c in s:\n",
    "            return c\n",
    "        \n",
    "def get_wood(s):\n",
    "    \"\"\"\n",
    "    Get neckwood\n",
    "    \"\"\"\n",
    "    # TODO: improve this function by getting the wood for the different parts:\n",
    "    #eg:\n",
    "    #   Top: Basswood\n",
    "    #   Body: Basswood\n",
    "    #   Neck: Mahogany\n",
    "    #   Fingerboard: Rosewood\n",
    "    #   Nut: Bone\n",
    "    \n",
    "    woods = [\"alder\",\"basswod\",\"magahony\",\"swamp ash\", \"walnut\",\"koa\",\"mapple\",\"rosewood\",\"ebony\",\"wenge\"]\n",
    "    result = \"\"\n",
    "    s=s.lower()\n",
    "    for wood in woods:\n",
    "        if wood in s:\n",
    "            return wood\n",
    "    if \"neck wood\" in s:\n",
    "        split_on_neck_wood = s.split(\"neck wood\")\n",
    "    return result\n",
    "\n",
    "def find_relevant_information(content,title,url,post_date):\n",
    "    \"\"\"\n",
    "    This is the main function, scrapping the price, vintage etc from title and content and returning a pd.DF row (list)\n",
    "    \"\"\"\n",
    "    # [\"brand\",\"model\",\"price\",\"vintage\",\"color\",\"wood\",\"post_date\", \"post_title\",\"post_content\",\"post_url\"]\n",
    "    new_row = []\n",
    "    \n",
    "    # find vintage\n",
    "    vintage = find_vintage(title)\n",
    "    if vintage ==\"\":\n",
    "        vintage = find_vintage(content)\n",
    "    # find price\n",
    "    price = find_price(content)\n",
    "\n",
    "    # If we have price, find brand, model, wood, color\n",
    "    if price!=\"\":\n",
    "\n",
    "        # find brand\n",
    "        brand = get_brand(content)\n",
    "        if brand==\"\":\n",
    "            brand = get_brand(title)\n",
    "        # find model\n",
    "        model = get_model(content, brand)\n",
    "        if model==\"\":\n",
    "            model = get_model(content, brand)\n",
    "        # find color\n",
    "        color = find_color(content)\n",
    "        # find wood\n",
    "        wood = get_wood(content)\n",
    "\n",
    "        # create a new pd.Df row and insert it if not only there\n",
    "        new_row=[brand,model,price, vintage,color,wood,post_date,title,content,url,]\n",
    "        \n",
    "    return new_row\n",
    "\n",
    "def scrap_brands_from_wikipedia():\n",
    "    \"\"\"\n",
    "    Scrapp all guitar brands from wikipedia and saves it into a file\n",
    "    \"\"\"\n",
    "    brands = []\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/List_of_guitar_manufacturers\"\n",
    "    wikipedia_rep = requests.get(wikipedia_url)\n",
    "    wikipedia_soup = bs4.BeautifulSoup(wikipedia_rep.text, 'html.parser')\n",
    "    lis = wikipedia_soup.findAll(\"div\",{\"class\": \"div-col\", \"style\":\"column-width: 22em;\"})[0].findAll(\"li\")\n",
    "    for li in lis:\n",
    "        a_s = li.findAll(\"a\")\n",
    "        if len(a_s)>0:\n",
    "            if a_s[0].has_attr('title'):\n",
    "                title = str(a_s[0][\"title\"]).lower()\n",
    "                # machin (bidule) -> machin\n",
    "                title = title.split(\" (\")[0]\n",
    "                # machin guitars -> machin\n",
    "                title = title.split(\" guitars\")[0]\n",
    "                title = title.split(\" guitar\")[0]\n",
    "                # machin music -> machin\n",
    "                title = title.split(\" music\")[0]\n",
    "                brands.append(title)\n",
    "    pd.DataFrame(brands, columns=[\"Brand\"]).to_csv(path+\"guitar_brands.csv\", index=False)\n",
    "    return brands\n",
    "\n",
    "def kdb_format_year(year):\n",
    "    year = str(year)\n",
    "    y = \"\"\n",
    "    for char in year:\n",
    "        if char.isdigit():\n",
    "            y+=char\n",
    "    year=y\n",
    "    if year==\"None\" or year ==\"\":\n",
    "        return \"0Ni\"\n",
    "    else:\n",
    "        return year+\"i\"\n",
    "    \n",
    "def insert_scrappedGuitars_into_kdb(df):\n",
    "    \"\"\"\n",
    "    Insert all rows of the data frame into kdb\n",
    "    Note:The roll to hdb is fully handled in q\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        post_date = str(row[\"post_date\"])\n",
    "        brand = str(row[\"brand\"])\n",
    "        model = str(row[\"model\"])\n",
    "        price = str(float(row[\"price\"].replace(\",\",\"\")))\n",
    "        vintage = kdb_format_year(str(row[\"vintage\"]))\n",
    "        color = str(row[\"color\"])\n",
    "        neckWood = str(row[\"neckWood\"])\n",
    "        pickupType = str(row[\"pickupType\"])\n",
    "        website = str(row[\"website\"])\n",
    "        post_title = str(row[\"post_title\"]).replace(\"\\\"\",\"\").replace(u\"\\u2019\",\" \")\n",
    "        post_title = ''.join([i if ord(i) < 128 else ' ' for i in post_title])\n",
    "        post_id = \"_\".join([post_date,website,brand,model,price,vintage]).replace(\".\",\"\")\n",
    "        \n",
    "        kdb_row  = post_date+\";\"+\\\n",
    "            \"`$\\\"\"+post_id+\"\\\";\"+\\\n",
    "            \"`$\\\"\"+brand+\"\\\"\"+\";\"+\\\n",
    "            \"`$\\\"\"+model+\"\\\"\"+\";\"+\\\n",
    "            price+\";\"+\\\n",
    "            vintage+\";\"+\\\n",
    "            \"`$\\\"\"+color+\"\\\"\"+\";\"+\\\n",
    "            \"`$\\\"\"+neckWood+\"\\\"\"+\";\"+\\\n",
    "            \"`$\\\"\"+pickupType+\"\\\"\"+\";\"+\\\n",
    "            \"`$\\\"\"+website+\"\\\"\"+\";\"+\\\n",
    "            \"\\\"\"+str(row[\"post_url\"])+\"\\\"\"+\";\"+\\\n",
    "            \"\\\"\"+post_title+\"\\\"\"\n",
    "\n",
    "        with qconnection.QConnection(host = \"localhost\",port=41643, username=\"n1ch3\", password=\"n1ch3\", pandas = True) as q:\n",
    "            try:\n",
    "                X=q('upsert[`scrappedGuitarsToday;('+ kdb_row +')]')\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                print('('+ kdb_row +')')\n",
    "                print(\"___\")\n",
    "                break\n",
    "                                \n",
    "def init_kdb_instance():\n",
    "    init=\"\"\"\n",
    "    scrappedGuitarsToday:([] date:`date$();id:`symbol$();brand:`symbol$();model:`symbol$();price:`float$();vintage:`int$();color:`symbol$();neckWood:`symbol$();pickupType:`symbol$();website:`symbol$();url:();title:());\n",
    "    recommendedGuitarsToday:([] date:`date$();id:`symbol$());\n",
    "    \n",
    "    hdbDirectory : `$\"c:/dev/niche/kdb\";\n",
    "    rollToHdb:{[hdbDirectory;data;targetDate]\n",
    "        show \"saving to hdb for date:\",string targetDate;\n",
    "        system \"l \",string hdbDirectory;\n",
    "        $[`scrappedGuitars in tables[];\n",
    "            [\n",
    "                show \"check there is no id already\";\n",
    "                ids:exec id from select distinct id from scrappedGuitars;\n",
    "                delete scrappedGuitars from `.;\n",
    "                dataTargetDate:select from data where date=targetDate, not id in ids;\n",
    "                ];\n",
    "            [\n",
    "                show \"Passing here\";\n",
    "                dataTargetDate:select from data where date=targetDate;\n",
    "                ]\n",
    "            ];\n",
    "        show \"n:\",string count dataTargetDate;\n",
    "        if[0<count dataTargetDate;\n",
    "            show \"Persisting!\";\n",
    "            `scrappedGuitars set `brand`model xasc delete date from dataTargetDate;\n",
    "            .Q.dpft[hsym hdbDirectory;targetDate;`brand;`scrappedGuitars]\n",
    "            ];\n",
    "        };\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with qconnection.QConnection(host = \"localhost\",port=41643, username=\"n1ch3\", password=\"n1ch3\", pandas = True) as q:\n",
    "            try:\n",
    "                X=q(init)\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "\n",
    "def roll_to_HDB_all_dates():\n",
    "    \"\"\"\n",
    "    Rolls all dates of the data in the table to hdb\n",
    "    \"\"\"\n",
    "    roll = \"rollToHdb[hdbDirectory;scrappedGuitarsToday;] each exec distinct date from scrappedGuitarsToday;\"\n",
    "    with qconnection.QConnection(host = \"localhost\",port=41643, username=\"n1ch3\", password=\"n1ch3\", pandas = True) as q:\n",
    "        try:\n",
    "            X=q(roll)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "def roll_to_HDB_today_only():\n",
    "    \"\"\"\n",
    "    Only saves data for today's date to hdb\n",
    "    \"\"\"\n",
    "    roll = \"rollToHdb[hdbDirectory;scrappedGuitarsToday;.z.D]\"\n",
    "    with qconnection.QConnection(host = \"localhost\",port=41643, username=\"n1ch3\", password=\"n1ch3\", pandas = True) as q:\n",
    "        try:\n",
    "            X=q(roll)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "def get_average_and_median_price_for_guitar(filters):\n",
    "    \"\"\"\n",
    "    :filters: dict, containing at least keys:\"brand\" and \"model\"\n",
    "    \"\"\"\n",
    "    query = \"select from scrappedGuitars where date<.z.d\"#, brand=`\"+filters[\"brand\"]+\", model=`$\\\"\"+filters[\"model\"]+\"\\\"\"\n",
    "    for k in filters.keys():\n",
    "        query+=\",\"+k+\"=\"+filters[k]\n",
    "    print(query)\n",
    "    with qconnection.QConnection(host = \"localhost\",port=41643, username=\"n1ch3\", password=\"n1ch3\", pandas = True) as q:\n",
    "        try:\n",
    "            guitars=q(query)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    mean = guitars[\"price\"].mean()\n",
    "    median = guitars[\"price\"].median()\n",
    "    n = len(guitars[\"price\"])\n",
    "    return [n,mean,median]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrap The Gear Page, processing the first 50 pages.\n",
      "... 2.0% done\n",
      "... 4.0% done\n",
      "... 6.0% done\n",
      "... 8.0% done\n",
      "... 10.0% done\n",
      "... 12.0% done\n",
      "... 14.0% done\n",
      "... 16.0% done\n",
      "... 18.0% done\n",
      "... 20.0% done\n",
      "... 22.0% done\n",
      "... 24.0% done\n",
      "could not convert string to float: \n",
      "\n",
      "... 26.0% done\n",
      "could not convert string to float: \n",
      "\n",
      "... 28.0% done\n",
      "... 30.0% done\n",
      "... 32.0% done\n",
      "... 34.0% done\n",
      "... 36.0% done\n",
      "... 38.0% done\n",
      "... 40.0% done\n",
      "... 42.0% done\n",
      "... 44.0% done\n",
      "... 46.0% done\n",
      "... 48.0% done\n",
      "... 50.0% done\n",
      "could not convert string to float: \n",
      "\n",
      "... 52.0% done\n",
      "... 54.0% done\n",
      "... 56.0% done\n",
      "... 58.0% done\n",
      "... 60.0% done\n",
      "... 62.0% done\n",
      "... 64.0% done\n",
      "... 66.0% done\n",
      "... 68.0% done\n",
      "... 70.0% done\n",
      "... 72.0% done\n",
      "... 74.0% done\n",
      "... 76.0% done\n",
      "... 78.0% done\n",
      "... 80.0% done\n",
      "No more After pages to scrap, stopping here :)\n"
     ]
    }
   ],
   "source": [
    "df_reddit=pd.DataFrame(columns=[\"brand\",\"model\",\"price\",\"vintage\",\"color\",\"wood\",\"post_date\",\"post_title\",\"post_content\",\"post_url\"])\n",
    "k=0\n",
    "n_pages = 50\n",
    "\n",
    "print(\"Starting to scrap The Gear Page, processing the first \"+str(n_pages)+\" pages.\")\n",
    "after = \"\"\n",
    "for page_number in [i+1 for i in range(n_pages)]:\n",
    "    url = 'https://www.reddit.com/r/Gear4Sale/.json'\n",
    "    \n",
    "    if after==None:\n",
    "        print(\"No more After pages to scrap, stopping here :)\")\n",
    "        break\n",
    "    if after!=\"\":\n",
    "        url += '?&after='+after\n",
    "    \n",
    "    r = requests.get(\n",
    "        url,\n",
    "        headers={'user-agent': 'Mozilla/5.0'})\n",
    "    \n",
    "    data = r.json()['data']\n",
    "    all_posts =data['children']\n",
    "    after = data[\"after\"]\n",
    "\n",
    "    for post in all_posts:\n",
    "        try:\n",
    "            title = post[\"data\"][\"title\"]\n",
    "            if any(ext in title for ext in [\"sell\", \"wts\",\"Sell\",\"WTS\"]):\n",
    "                title = post[\"data\"][\"title\"]\n",
    "                content = post[\"data\"][\"selftext\"]\n",
    "                url = post[\"data\"][\"url\"]\n",
    "                post_date = datetime.datetime.fromtimestamp(float(post[\"data\"][\"created\"])).strftime(\"%Y.%m.%d\")\n",
    "                # create a new pd.Df row and insert it if not only there\n",
    "                new_row = find_relevant_information(content,title,url,post_date)\n",
    "                if new_row!=[] and not ((df_reddit['post_title'] == title) & (df_reddit['post_content'] == content)).any():\n",
    "                    df_reddit.loc[k]=new_row\n",
    "                    k+=1\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    r.close()\n",
    "    print(\"... \"+str(round(page_number/n_pages*100,2))+\"% done\")\n",
    "\n",
    "df_reddit[\"website\"]=\"Reddit\"\n",
    "df_reddit.to_csv(path+datetime.datetime.now().strftime(\"%Y_%m_%d\")+\"_reddit.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gear Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrap The Gear Page, processing the first 50 pages.\n",
      "... 2.0% done\n",
      "... 4.0% done\n",
      "... 6.0% done\n",
      "... 8.0% done\n",
      "... 10.0% done\n",
      "... 12.0% done\n",
      "... 14.0% done\n",
      "... 16.0% done\n",
      "... 18.0% done\n",
      "... 20.0% done\n",
      "... 22.0% done\n",
      "... 24.0% done\n",
      "... 26.0% done\n",
      "... 28.0% done\n",
      "... 30.0% done\n",
      "... 32.0% done\n",
      "... 34.0% done\n",
      "... 36.0% done\n",
      "... 38.0% done\n",
      "... 40.0% done\n",
      "could not convert string to float: \n",
      "\n",
      "... 42.0% done\n",
      "... 44.0% done\n",
      "... 46.0% done\n",
      "... 48.0% done\n",
      "... 50.0% done\n",
      "... 52.0% done\n",
      "... 54.0% done\n",
      "... 56.0% done\n",
      "... 58.0% done\n",
      "... 60.0% done\n",
      "... 62.0% done\n",
      "... 64.0% done\n",
      "... 66.0% done\n",
      "... 68.0% done\n",
      "... 70.0% done\n",
      "... 72.0% done\n",
      "... 74.0% done\n",
      "... 76.0% done\n",
      "... 78.0% done\n",
      "... 80.0% done\n",
      "... 82.0% done\n",
      "... 84.0% done\n",
      "... 86.0% done\n",
      "... 88.0% done\n",
      "... 90.0% done\n",
      "... 92.0% done\n",
      "... 94.0% done\n",
      "... 96.0% done\n",
      "... 98.0% done\n",
      "... 100.0% done\n",
      "Succesfully scraped The Gear Page, saving result to file: C:/dev/niche/guitar/data/2021_01_19_theGearPage.csv\n"
     ]
    }
   ],
   "source": [
    "tgp_base_url = \"https://www.thegearpage.net\"\n",
    "tgp_url = \"https://www.thegearpage.net/board/index.php?forums/guitar-emporium.22/\"\n",
    "df_tgp=pd.DataFrame(columns=[\"brand\",\"model\",\"price\",\"vintage\",\"color\",\"wood\",\"post_date\",\"post_title\",\"post_content\",\"post_url\"])\n",
    "k=0\n",
    "n_pages = 50\n",
    "print(\"Starting to scrap The Gear Page, processing the first \"+str(n_pages)+\" pages.\")\n",
    "\n",
    "for page_number in [i+1 for i in range(n_pages)]:\n",
    "    url = tgp_url + \"page-\"+str(page_number)\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    for div in soup.findAll(\"div\", {\"class\":\"structItem-cell structItem-cell--main\"}):\n",
    "        try:\n",
    "            # check that it is a selling post\n",
    "            if any(tag in div.text.upper() for tag in [\"FS\" or \"FOS\" or \"WTS\" or \"SELL\" or \"SALE\"]):\n",
    "                title = div.findAll(\"a\", {\"class\":\"\"})[0].text\n",
    "\n",
    "                # get the post url and from it the content\n",
    "                post_url = tgp_base_url+div.findAll(\"a\")[1][\"href\"]\n",
    "                post_rep = requests.get(post_url)\n",
    "                post_soup = bs4.BeautifulSoup(post_rep.text, 'html.parser')\n",
    "                post_date = datetime.datetime.fromtimestamp(float(post_soup.findAll(\"time\",{\"class\":\"u-dt\"})[0][\"data-time\"])).strftime(\"%Y.%m.%d\")\n",
    "                bbWrappers = post_soup.findAll(\"div\", {\"class\":\"bbWrapper\"})\n",
    "                content = bbWrappers[0].text\n",
    "                post_rep.close()\n",
    "\n",
    "                new_row = find_relevant_information(content,title,post_url,post_date)\n",
    "                if new_row!=[] and not ((df_tgp['post_title'] == title) & (df_tgp['post_content'] == content)).any():\n",
    "                    df_tgp.loc[k]=new_row\n",
    "                    k+=1\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    r.close()\n",
    "    print(\"... \"+str(round(page_number/n_pages*100,2))+\"% done\")\n",
    "\n",
    "scrapping_results_file_name = path+datetime.datetime.now().strftime(\"%Y_%m_%d\")+\"_theGearPage.csv\"\n",
    "print(\"Succesfully scraped The Gear Page, saving result to file: \"+scrapping_results_file_name)\n",
    "\n",
    "df_tgp[\"website\"]=\"TheGearPage\"\n",
    "df_tgp.to_csv(scrapping_results_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_kdb_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_scrappedGuitars_into_kdb(df_tgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_scrappedGuitars_into_kdb(df_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_to_HDB_all_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_and_median_price_for_guitar({\"brand\":\"`fender\",\"model\":\"`$\\\"\\\"\", \"vintage\":\"2017\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
